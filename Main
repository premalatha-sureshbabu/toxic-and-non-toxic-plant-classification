import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
# dataset link:https://www.kaggle.com/datasets/hanselliott/toxic-plant-classification
# This code snippet appears to be a comment and doesn't have any syntax errors.

pip install kaggle

%env KAGGLE_CONFIG_DIR=<path-to-kaggle-json-folder>

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d hanselliott/toxic-plant-classification

import zipfile
import os

# Check if the file exists with a slightly different name due to Kaggle download
if os.path.exists('toxic-plant-classification.zip'):
    dataset_zip = 'toxic-plant-classification.zip'
else:
    dataset_zip = './new-plant-classification-dataset.zip' # Keep the original name if the above doesn't exist

extract_dir = './plant_classification_dataset/'

# Create the directory if it doesn't exist
if not os.path.exists(extract_dir):
    os.makedirs(extract_dir)

# Extract the zip file
with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)


import zipfile
import os
import glob

# Use glob to find zip files matching the pattern
dataset_zip = glob.glob('*plant*classification*.zip')[0]

extract_dir = './plant_classification_dataset/'

# Create the directory if it doesn't exist
if not os.path.exists(extract_dir):
    os.makedirs(extract_dir)

# Extract the zip file
with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)


!unzip -o toxic-plant-classification.zip -d ./plant_classification_dataset/

import os

# Path to the extracted dataset
dataset_dir = './plant_classification_dataset/'

# Print the directories and files in the dataset
for root, dirs, files in os.walk(dataset_dir):
    print(root)
    if dirs:
        print("Subdirectories:", dirs)
    if files:
        print("Files:", files)
    break  # Print only the top-level directory

!ls plant_classification_dataset


import tensorflow as tf

# Ensure the path is correct and points to the training data
training_set = tf.keras.utils.image_dataset_from_directory(
    './plant_classification_dataset/', # Updated path to point to the actual location of your data
    labels="inferred",
    label_mode="categorical",
    class_names=None,
    color_mode="rgb",
    batch_size=32,
    image_size=(128, 128),
    shuffle=True,
    seed=None,
    validation_split=None,
    subset=None,
    interpolation="bilinear",
    follow_links=False,
    crop_to_aspect_ratio=False
)



import tensorflow as tf

# Ensure the path is correct and points to the training data
validation_set = tf.keras.utils.image_dataset_from_directory(
    './plant_classification_dataset/', # Changed path to point to the actual location of your data
    labels="inferred",
    label_mode="categorical",
    class_names=None,
    color_mode="rgb",
    batch_size=32,
    image_size=(128, 128),
    shuffle=True,
    seed=123, # Added a seed value for consistent shuffling
    validation_split=0.2, # Added validation split
    subset='validation', # Specified subset as validation
    interpolation="bilinear",
    follow_links=False,
    crop_to_aspect_ratio=False
)


cnn = tf.keras.models.Sequential()


cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,padding='same',activation='relu',input_shape=[128,128,3]))
cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu'))
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))


cnn.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,padding='same',activation='relu'))
cnn.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,activation='relu'))
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))


cnn.add(tf.keras.layers.Conv2D(filters=128,kernel_size=3,padding='same',activation='relu'))
cnn.add(tf.keras.layers.Conv2D(filters=128,kernel_size=3,activation='relu'))
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))

cnn.add(tf.keras.layers.Conv2D(filters=256,kernel_size=3,padding='same',activation='relu'))
cnn.add(tf.keras.layers.Conv2D(filters=256,kernel_size=3,activation='relu'))
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))

cnn.add(tf.keras.layers.Conv2D(filters=512,kernel_size=3,padding='same',activation='relu'))
cnn.add(tf.keras.layers.Conv2D(filters=512,kernel_size=3,activation='relu'))
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))

cnn.add(tf.keras.layers.Dropout(0.25))

cnn.add(tf.keras.layers.Flatten())

cnn.add(tf.keras.layers.Dense(units=1500,activation='relu'))

cnn.add(tf.keras.layers.Dropout(0.4)) #To avoid overfitting

#Output Layer
cnn.add(tf.keras.layers.Dense(units=38,activation='softmax'))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# Initialize the CNN
cnn = Sequential()

# Add a convolutional layer with 32 filters and 3x3 kernel
cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=(128, 128, 3)))

# Add a pooling layer to reduce dimensions
cnn.add(MaxPooling2D(pool_size=2))

# Add more convolutional and pooling layers if needed (example with one more layer)
cnn.add(Conv2D(filters=64, kernel_size=3, activation='relu'))
cnn.add(MaxPooling2D(pool_size=2))

# Flatten the output from the convolutional layers
cnn.add(Flatten())

# Add a fully connected dense layer
cnn.add(Dense(units=128, activation='relu'))

# Add the output layer (with 38 classes)
cnn.add(Dense(units=38, activation='softmax'))

# Compile the CNN
cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the CNN
training_history = cnn.fit(x=training_set, validation_data=validation_set, epochs=10)

cnn.summary()

#Training set Accuracy
train_loss, train_acc = cnn.evaluate(training_set)
print('Training accuracy:', train_acc)

val_loss, val_acc = cnn.evaluate(validation_set)
print('Validation accuracy:', val_acc)

cnn.save('trained_toxic_plant_model.keras')


training_history.history

#Recording History in json
import json
with open('training_hist.json','w') as f:
  json.dump(training_history.history,f)


print(training_history.history.keys())

epochs = [i for i in range(1,11)]
plt.plot(epochs,training_history.history['accuracy'],color='red',label='Training Accuracy')
plt.plot(epochs,training_history.history['val_accuracy'],color='blue',label='Validation Accuracy')
plt.xlabel('No. of Epochs')
plt.title('Visualization of Accuracy Result')
plt.legend()
plt.show()


class_name = validation_set.class_names


import os

# Check contents of the current working directory
print("Contents of /content directory:", os.listdir('/content'))

# Check if 'valid' directory is present
valid_path = '/content/valid'
if os.path.isdir(valid_path):
    print(f"The 'valid' directory exists at: {valid_path}")
else:
    print(f"The 'valid' directory does not exist at: {valid_path}")


import zipfile
import os

# Path to the zip file
zip_path = '/content/toxic-plant-classification.zip'
extract_path = '/content/'

# Extract the zip file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# Check if the 'valid' directory has been extracted
print(os.listdir(extract_path))


valid_path = '/content/valid'
if os.path.isdir(valid_path):
    print("Contents of the 'valid' directory:", os.listdir(valid_path))
else:
    print("The 'valid' directory was not found.")


import zipfile

zip_path = '/content/toxic-plant-classification.zip'

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    # List all files in the zip
    zip_files = zip_ref.namelist()
    print("Contents of the zip file:", zip_files)


import zipfile
import os

extract_path = '/content/toxic-plant-dataset'

# Extract zip file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# List contents of the extraction directory
print(os.listdir(extract_path))


import os

# Check the contents of the extraction path
extracted_dir = '/content/toxic-plant-dataset'
print("Contents of the extracted directory:", os.listdir(extracted_dir))


import zipfile

zip_path = '/content/toxic-plant-classification.zip'

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    # List all files in the zip
    zip_files = zip_ref.namelist()
    print("Contents of the zip file:", zip_files)


# Example of listing directories and files in the extraction path
for root, dirs, files in os.walk(extracted_dir):
    print(f"Current directory: {root}")
    print("Subdirectories:", dirs)
    print("Files:", files)
    print()


dataset_path = '/content/toxic-plant-dataset/tpc-imgs/toxic_images'  # Adjust this path based on findings

test_set = tf.keras.utils.image_dataset_from_directory(
    dataset_path,
    labels="inferred",
    label_mode="categorical",
    color_mode="rgb",
    batch_size=1,
    image_size=(128, 128),
    shuffle=False
)



y_pred = cnn.predict(test_set)
predicted_categories = tf.argmax(y_pred, axis=1)


true_categories = tf.concat([y for x, y in test_set], axis=0)
Y_true = tf.argmax(true_categories, axis=1)


Y_true

predicted_categories

from sklearn.metrics import confusion_matrix,classification_report
cm = confusion_matrix(Y_true,predicted_categories)


# Assuming your class names are "class1", "class2", ..., "class5"
class_names = ["class1", "class2", "class3", "class4", "class5"]

print(classification_report(Y_true, predicted_categories, target_names=class_names))


import tensorflow as tf

# Ensure the path is correct and points to the training data
validation_set = tf.keras.utils.image_dataset_from_directory(
    './plant_classification_dataset/', # Changed path to point to the actual location of your data
    labels="inferred",
    label_mode="categorical",
    class_names=None,
    color_mode="rgb",
    batch_size=32,
    image_size=(128, 128),
    shuffle=True,
    seed=123, # Added a seed value for consistent shuffling
    validation_split=0.2, # Added validation split
    subset='validation', # Specified subset as validation
    interpolation="bilinear",
    follow_links=False,
    crop_to_aspect_ratio=False
)

class_name = validation_set.class_names
print(class_name)

cnn = tf.keras.models.load_model('trained_toxic_plant_model.keras')

import cv2
from matplotlib import pyplot as plt
import os

# Option 1: Change the working directory (if needed)
# os.chdir('/path/to/the/directory/containing/tpc-imgs')

# Option 2: Use the full relative path
image_path = 'tpc-imgs/toxic_images/004/985.jpg'

# Reading an image in default mode
img = cv2.imread(image_path)

if img is None:
    print(f"Error: Could not load image at {image_path}")
else:
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    plt.imshow(img)
    plt.title('Test Image')
    plt.xticks([])
    plt.yticks([])
    plt.show()


import cv2
from matplotlib import pyplot as plt
import os
import numpy as np #Added import statement for numpy

# Option 1: Change the working directory (if needed)
# os.chdir('/path/to/the/directory/containing/tpc-imgs')

# Option 2: Use the full relative path
image_path = 'tpc-imgs/toxic_images/004/985.jpg'

# Reading an image in default mode
img = cv2.imread(image_path)

if img is None:
    print(f"Error: Could not load image at {image_path}")
else:
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    plt.imshow(img)
    plt.title('Test Image')
    plt.xticks([])
    plt.yticks([])
    plt.show()

image = tf.keras.preprocessing.image.load_img(image_path,target_size=(128,128))
input_arr = tf.keras.preprocessing.image.img_to_array(image)
input_arr = np.array([input_arr])  # Convert single image to a batch.
predictions = cnn.predict(input_arr)

print(predictions)


result_index = np.argmax(predictions) #Return index of max element
print(result_index)

# Displaying the plant prediction
model_prediction = class_name[0] # Changed index to 0 to access the first element in the list
plt.imshow(img)
plt.title(f"plant Name: {model_prediction}")
plt.xticks([])
plt.yticks([])
plt.show()
